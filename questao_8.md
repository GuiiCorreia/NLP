# Modelo de Linguagem com Janelas Deslizantes

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa um modelo de linguagem simples baseado em redes neurais feedforward usando a tÃ©cnica de **janelas deslizantes** (sliding windows). O objetivo Ã© analisar como o tamanho da janela de contexto afeta a perplexidade do modelo.

## ğŸ¯ Objetivo da QuestÃ£o 8

Implementar um modelo de linguagem neural feedforward que:
- Utiliza janelas deslizantes para capturar contexto
- Prediz a prÃ³xima palavra baseado nas N palavras anteriores
- Analisa como a perplexidade varia com diferentes tamanhos de janela

## ğŸ—ï¸ Arquitetura do Modelo

### Componentes Principais

1. **Camada de Embedding**
   - Converte palavras em vetores densos de dimensÃ£o fixa
   - Permite capturar similaridades semÃ¢nticas

2. **Camadas Feedforward**
   - Concatena os embeddings da janela de contexto
   - Camada oculta com ReLU e Dropout
   - Camada de saÃ­da com dimensÃ£o igual ao vocabulÃ¡rio

3. **FunÃ§Ã£o de Perda**
   - Cross-Entropy Loss para classificaÃ§Ã£o multiclasse
   - Cada palavra do vocabulÃ¡rio Ã© uma classe

### EquaÃ§Ãµes do Modelo

```
Entrada: [wâ‚, wâ‚‚, ..., wâ‚™] â†’ Embedding â†’ [eâ‚, eâ‚‚, ..., eâ‚™]

h = ReLU(Wâ‚ Â· concat(eâ‚, eâ‚‚, ..., eâ‚™) + bâ‚)

y = softmax(Wâ‚‚ Â· h + bâ‚‚)
```

Onde:
- `n` = tamanho da janela
- `eáµ¢` = embedding da palavra i
- `h` = representaÃ§Ã£o oculta
- `y` = distribuiÃ§Ã£o de probabilidade sobre o vocabulÃ¡rio

## ğŸ“Š Janelas Deslizantes

### Como Funciona

Para o texto: "the cat sat on the mat"

Com janela de tamanho 3:
```
Janela 1: [the, cat, sat] â†’ Prediz: on
Janela 2: [cat, sat, on] â†’ Prediz: the
Janela 3: [sat, on, the] â†’ Prediz: mat
```

### Vantagens
- âœ… Captura dependÃªncias locais
- âœ… Eficiente computacionalmente
- âœ… Simples de implementar

### LimitaÃ§Ãµes
- âŒ Contexto fixo e limitado
- âŒ NÃ£o captura dependÃªncias de longo alcance
- âŒ Precisa de mais parÃ¢metros para janelas maiores

## ğŸ“ˆ Perplexidade

### DefiniÃ§Ã£o

A perplexidade mede quÃ£o "surpreso" o modelo fica com os dados de teste:

```
Perplexidade = exp(- (1/N) Î£ log P(wáµ¢|contexto))
```

### InterpretaÃ§Ã£o

- **Perplexidade baixa**: Modelo confiante e preciso
- **Perplexidade alta**: Modelo incerto ou impreciso
- **Perplexidade de N**: Equivalente a escolher aleatoriamente entre N palavras

### Exemplo

Se o modelo tem perplexidade 50:
- Ã‰ como se estivesse escolhendo entre 50 palavras igualmente provÃ¡veis
- Quanto menor, melhor o modelo

## ğŸ”¬ AnÃ¡lise: Tamanho da Janela vs Perplexidade

### Experimento

O cÃ³digo treina modelos com janelas de tamanho 2, 3, 4, 5 e 6 palavras.

### Resultados Esperados

#### Janela Pequena (2-3 palavras)
- **Perplexidade**: Alta
- **Motivo**: Contexto insuficiente para boas prediÃ§Ãµes
- **Exemplo**: "the cat" â†’ difÃ­cil prever se Ã© "sat", "ran", "jumped", etc.

#### Janela MÃ©dia (4-5 palavras)
- **Perplexidade**: ReduÃ§Ã£o significativa
- **Motivo**: Contexto suficiente para capturar padrÃµes
- **Exemplo**: "the cat sat on the" â†’ mais provÃ¡vel prever "mat" ou "floor"

#### Janela Grande (6+ palavras)
- **Perplexidade**: Diminui menos ou estabiliza
- **Motivo**: Overfitting ou dados insuficientes
- **Trade-off**: Mais parÃ¢metros vs quantidade de dados

### GrÃ¡fico TÃ­pico

```
Perplexidade
    |
150 |  â€¢
    |
120 |    â€¢
    |
 90 |      â€¢
    |
 60 |        â€¢
    |
 30 |          â€¢ ___________
    |________________________
      2   3   4   5   6   â†’ Tamanho da Janela
```

## ğŸš€ Como Executar

### Requisitos

```bash
pip install torch numpy matplotlib
```

### ExecuÃ§Ã£o

```bash
python language_model.py
```

### SaÃ­da

O programa irÃ¡:
1. Treinar modelos com diferentes tamanhos de janela
2. Imprimir a perplexidade ao longo do treinamento
3. Gerar grÃ¡ficos comparativos
4. Salvar visualizaÃ§Ãµes em `perplexity_analysis.png`

## ğŸ“ Estrutura do CÃ³digo

```
language_model.py
â”œâ”€â”€ TextDataset          # Cria janelas deslizantes do texto
â”œâ”€â”€ FeedForwardLM        # Arquitetura da rede neural
â”œâ”€â”€ build_vocab          # ConstrÃ³i vocabulÃ¡rio do texto
â”œâ”€â”€ calculate_perplexity # Calcula mÃ©trica de avaliaÃ§Ã£o
â”œâ”€â”€ train_model          # Loop de treinamento
â””â”€â”€ main                 # Experimentos e visualizaÃ§Ã£o
```

## ğŸ”§ HiperparÃ¢metros

```python
embedding_dim = 64      # DimensÃ£o dos embeddings
hidden_dim = 128        # DimensÃ£o da camada oculta
batch_size = 32         # Tamanho do batch
epochs = 30             # NÃºmero de Ã©pocas
learning_rate = 0.001   # Taxa de aprendizado
dropout = 0.2           # Taxa de dropout
```

## ğŸ“ CustomizaÃ§Ã£o

### Usar seu prÃ³prio texto

Substitua a variÃ¡vel `sample_text` no cÃ³digo:

```python
with open('seu_texto.txt', 'r', encoding='utf-8') as f:
    sample_text = f.read()
```

### Testar outros tamanhos de janela

Modifique a lista:

```python
window_sizes = [1, 2, 3, 4, 5, 6, 7, 8]
```

### Ajustar arquitetura

```python
# Modelo mais profundo
embedding_dim = 128
hidden_dim = 256

# Adicionar mais camadas no FeedForwardLM
self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
self.fc3 = nn.Linear(hidden_dim // 2, vocab_size)
```

## ğŸ“Š InterpretaÃ§Ã£o dos Resultados

### ComparaÃ§Ã£o com os Slides

Baseado nos slides de Stanford (Jurafsky & Martin):

1. **Modelos N-gram vs Neural**
   - N-grams: Contam ocorrÃªncias diretas
   - Neural: Aprendem representaÃ§Ãµes contÃ­nuas

2. **Vantagens do Modelo Neural**
   - GeneralizaÃ§Ã£o para palavras similares
   - Embeddings capturam semÃ¢ntica
   - Menos esparsidade

3. **Trade-offs**
   - Janela maior â†’ Mais contexto â†’ Menos perplexidade
   - Janela maior â†’ Mais parÃ¢metros â†’ Risco de overfitting
   - Janela maior â†’ Menos exemplos de treino

## ğŸ“ Conceitos Importantes

### 1. Problema de Esparsidade
- N-grams sofrem com combinaÃ§Ãµes nÃ£o vistas
- Redes neurais generalizam via embeddings

### 2. MaldiÃ§Ã£o da Dimensionalidade
- Janelas maiores aumentam o espaÃ§o de busca
- Requer mais dados de treinamento

### 3. LimitaÃ§Ãµes do Feedforward
- Contexto fixo (nÃ£o como RNNs/Transformers)
- NÃ£o compartilha pesos entre posiÃ§Ãµes
- NÃ£o processa sequÃªncias de tamanho variÃ¡vel

## ğŸ“š ReferÃªncias

1. Jurafsky & Martin - Speech and Language Processing
   - [Slides de Redes Neurais](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)
   - [Slides de Language Models](https://web.stanford.edu/~jurafsky/slp3/slides/lm_jan25.pdf)

2. Bengio et al. (2003) - "A Neural Probabilistic Language Model"
   - Primeiro modelo neural de linguagem feedforward

3. Goodfellow et al. - Deep Learning Book
   - CapÃ­tulo sobre Sequence Modeling

## ğŸ’¡ PrÃ³ximos Passos

Para melhorar o modelo:

1. **RNN/LSTM**: Contexto variÃ¡vel e memÃ³ria
2. **Transformers**: AtenÃ§Ã£o e paralelizaÃ§Ã£o
3. **Pre-training**: Transfer learning (BERT, GPT)
4. **RegularizaÃ§Ã£o**: Weight decay, early stopping
5. **Dados**: Corpus maior (ex: WikiText, Penn Treebank)

## ğŸ¤ ConclusÃ£o

Este projeto demonstra:
- âœ… ImplementaÃ§Ã£o de modelo neural de linguagem
- âœ… AnÃ¡lise empÃ­rica de janelas deslizantes
- âœ… RelaÃ§Ã£o entre tamanho de contexto e perplexidade
- âœ… Fundamentos para arquiteturas mais avanÃ§adas

**Resultado esperado**: A perplexidade geralmente diminui Ã  medida que aumentamos o tamanho da janela, atÃ© um ponto de diminishing returns onde contextos muito grandes nÃ£o trazem ganhos significativos (especialmente com dados limitados).

