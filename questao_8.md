# ComparaÃ§Ã£o: Modelo de Bigramas vs Neural Network Language Model (NN-LM)

## ğŸ“‹ QuestÃ£o 8
**Compare um modelo de linguagem baseado em bigramas com um pequeno NN-LM no mesmo corpus. Em quais aspectos o NN-LM supera o modelo de n-gramas?**

## ğŸ¯ Objetivo
Este projeto implementa e compara dois modelos de linguagem:
1. **Modelo de Bigramas** com suavizaÃ§Ã£o Add-1 (Laplace)
2. **Neural Network Language Model (NN-LM)** simples com embeddings

## ğŸ“¦ DependÃªncias

### InstalaÃ§Ã£o
```bash
pip install numpy torch matplotlib
```

### Requisitos
- Python 3.7+
- NumPy
- PyTorch
- Matplotlib

## ğŸš€ Como Executar

### ExecuÃ§Ã£o BÃ¡sica
```bash
python comparacao_bigrama_nnlm.py
```

### O que o script faz:
1. **DemonstraÃ§Ã£o de GeneralizaÃ§Ã£o**: Mostra como NN-LM generaliza melhor para palavras nÃ£o vistas
2. **Tratamento de Zeros**: Compara como cada modelo lida com sequÃªncias nÃ£o vistas no treino
3. **ComparaÃ§Ã£o Completa**: Treina ambos os modelos e calcula perplexidade
4. **Gera PDF**: Cria um relatÃ³rio visual com grÃ¡ficos comparativos

### SaÃ­da
- **Console**: Resultados detalhados da comparaÃ§Ã£o
- **PDF**: `comparacao_bigrama_vs_nnlm.pdf` com visualizaÃ§Ãµes e anÃ¡lise completa

## ğŸ“Š Estrutura do CÃ³digo

### Classes Principais

#### `BigramModel`
Implementa modelo de linguagem baseado em bigramas:
- SuavizaÃ§Ã£o Add-1 (Laplace) para lidar com zeros
- CÃ¡lculo de probabilidade: P(wâ‚‚|wâ‚) = (C(wâ‚,wâ‚‚) + 1) / (C(wâ‚) + V)
- CÃ¡lculo de perplexidade no conjunto de teste

#### `SimpleNNLM`
Rede neural para modelagem de linguagem:
- **Arquitetura**: Embedding â†’ Hidden Layer (ReLU) â†’ Output (Softmax)
- **Embeddings**: RepresentaÃ§Ãµes vetoriais das palavras
- **Contexto**: Janela de N palavras anteriores

#### `NNLMWrapper`
Wrapper para facilitar treinamento e avaliaÃ§Ã£o:
- ConstruÃ§Ã£o de vocabulÃ¡rio
- PreparaÃ§Ã£o de dados
- Treinamento com Adam optimizer
- CÃ¡lculo de perplexidade

## ğŸ”¬ Experimentos Realizados

### 1. DemonstraÃ§Ã£o de GeneralizaÃ§Ã£o
```
Corpus de Treino:
- "the cat eats fish"
- "the dog eats meat"
- (sem "bird")

Teste:
- "the bird eats seeds"
```
**Resultado**: NN-LM generaliza melhor pois embeddings de "cat", "dog" e "bird" sÃ£o similares.

### 2. Problema de Zeros
```
Treino:
- "I like apples"
- "I eat oranges"
- (nunca "eat bananas" juntos)

Teste:
- "I eat bananas"
```
**Resultado**: Bigrama sofre mesmo com Add-1. NN-LM usa similaridade dos embeddings.

### 3. ComparaÃ§Ã£o de Perplexidade
Avalia ambos os modelos em corpus maior e calcula mÃ©tricas objetivas.

## ğŸ“ˆ Aspectos em que o NN-LM Supera N-gramas

### 1. âœ… GeneralizaÃ§Ã£o atravÃ©s de Embeddings
- **Bigramas**: Trata cada palavra independentemente
- **NN-LM**: Aprende representaÃ§Ãµes que capturam similaridade semÃ¢ntica
- **Exemplo**: "cat eats fish" â†’ pode generalizar para "dog eats meat"

### 2. âœ… Problema da Esparsidade
- **Bigramas**: Muitas combinaÃ§Ãµes nunca aparecem no treino (99%+ sÃ£o zeros)
- **NN-LM**: Embeddings compartilhados reduzem drasticamente a esparsidade
- **Resultado**: Melhor prediÃ§Ã£o para sequÃªncias nÃ£o vistas

### 3. âœ… RepresentaÃ§Ã£o ContÃ­nua vs Discreta
- **Bigramas**: EspaÃ§o discreto - palavras nÃ£o tÃªm relaÃ§Ã£o
- **NN-LM**: EspaÃ§o contÃ­nuo - palavras similares tÃªm vetores prÃ³ximos
- **Vantagem**: InterpolaÃ§Ã£o suave entre conceitos

### 4. âœ… PadrÃµes Complexos
- **Bigramas**: Apenas dependÃªncias locais (palavra anterior)
- **NN-LM**: Hidden layers capturam padrÃµes nÃ£o-lineares
- **BenefÃ­cio**: Features automÃ¡ticas vs contagem simples

### 5. âœ… Melhor Uso dos Dados
- **Bigramas**: Cada bigrama aprendido independentemente
- **NN-LM**: Compartilha conhecimento atravÃ©s de pesos e embeddings
- **Impacto**: Perplexidade menor = melhor modelagem

### 6. âœ… Escalabilidade
- **Bigramas**: Cresce O(VÂ²) com vocabulÃ¡rio
- **NN-LM**: Cresce O(V) no embedding
- **Resultado**: Mais eficiente para vocabulÃ¡rios grandes

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

### Perplexidade
```
PP(W) = P(wâ‚wâ‚‚...wâ‚™)^(-1/N)
```
- **Menor Ã© melhor**: Indica melhor prediÃ§Ã£o das palavras
- Normalizada pelo nÃºmero de palavras
- Range: [1, âˆ]

### Resultados TÃ­picos
- **Bigrama**: Perplexidade ~200-400
- **NN-LM**: Perplexidade ~50-150 (melhoria de 40-60%)

## ğŸ¨ VisualizaÃ§Ãµes Geradas

O PDF contÃ©m:
1. **GrÃ¡fico de Barras**: ComparaÃ§Ã£o direta de perplexidade
2. **Curva de Loss**: ConvergÃªncia do NN-LM durante treinamento
3. **AnÃ¡lise Textual**: ExplicaÃ§Ã£o detalhada de cada aspecto superior

## ğŸ”§ PersonalizaÃ§Ã£o

### Ajustar HiperparÃ¢metros
```python
# No cÃ³digo, vocÃª pode modificar:
nnlm = NNLMWrapper(
    embedding_dim=50,    # DimensÃ£o dos embeddings
    hidden_dim=100,      # Tamanho da camada oculta
    context_size=2       # Tamanho da janela de contexto
)

nnlm.train(
    train_corpus, 
    epochs=30,           # NÃºmero de Ã©pocas
    lr=0.005            # Taxa de aprendizado
)
```

### Usar Seu PrÃ³prio Corpus
```python
# Adicione suas prÃ³prias sentenÃ§as:
meu_corpus = [
    "sua primeira sentenÃ§a",
    "sua segunda sentenÃ§a",
    # ...
]

# Divida em treino/teste
train = meu_corpus[:int(0.7 * len(meu_corpus))]
test = meu_corpus[int(0.7 * len(meu_corpus)):]
```

## ğŸ“š ReferÃªncias

### Materiais Base
- [Stanford NLP - Neural Networks and Neural Language Models](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)
- [Stanford NLP - N-gram Language Modeling](https://web.stanford.edu/~jurafsky/slp3/slides/lm_jan25.pdf)

### Conceitos Chave
- **Bigramas**: P(wâ‚™|wâ‚™â‚‹â‚) â‰ˆ C(wâ‚™â‚‹â‚,wâ‚™) / C(wâ‚™â‚‹â‚)
- **Add-1 Smoothing**: P(wâ‚™|wâ‚™â‚‹â‚) = (C(wâ‚™â‚‹â‚,wâ‚™) + 1) / (C(wâ‚™â‚‹â‚) + V)
- **Neural LM**: Usa embeddings + feedforward network
- **Embeddings**: RepresentaÃ§Ãµes vetoriais densas que capturam semÃ¢ntica

## ğŸ’¡ Insights Principais

### Por que NN-LM Ã© Superior?

1. **Embeddings = MemÃ³ria SemÃ¢ntica**
   - Palavras similares tÃªm representaÃ§Ãµes similares
   - Permite transferÃªncia de conhecimento

2. **Compartilhamento de ParÃ¢metros**
   - Mesmo embedding usado em mÃºltiplos contextos
   - Aprendizado mais eficiente

3. **NÃ£o-linearidade**
   - FunÃ§Ãµes de ativaÃ§Ã£o (ReLU) capturam padrÃµes complexos
   - Vai alÃ©m de contagem simples

4. **GeneralizaÃ§Ã£o**
   - Pode inferir probabilidades para combinaÃ§Ãµes nÃ£o vistas
   - Reduz problema de sparsity

### LimitaÃ§Ãµes do Bigrama

1. **IndependÃªncia Condicional**
   - Assume que sÃ³ a palavra anterior importa
   - Ignora contexto mais amplo

2. **Esparsidade Severa**
   - 99%+ das combinaÃ§Ãµes possÃ­veis sÃ£o zero
   - Mesmo com smoothing, problemas persistem

3. **Sem SemÃ¢ntica**
   - "cat" e "dog" sÃ£o completamente independentes
   - NÃ£o captura similaridade

## ğŸ“ ConclusÃ£o

O NN-LM supera o modelo de bigramas principalmente devido Ã :
- **RepresentaÃ§Ã£o contÃ­nua** via embeddings
- **GeneralizaÃ§Ã£o semÃ¢ntica** para palavras similares
- **ReduÃ§Ã£o de sparsity** atravÃ©s de compartilhamento de parÃ¢metros
- **Capacidade nÃ£o-linear** de capturar padrÃµes complexos

Isso resulta em **perplexidade menor** e **melhor prediÃ§Ã£o** de palavras.
