# ComparaÃ§Ã£o EmpÃ­rica de FunÃ§Ãµes de AtivaÃ§Ã£o em Redes Neurais

## ğŸ“‹ QuestÃ£o 3

**Objetivo:** Compare empiricamente o desempenho de redes pequenas usando sigmoid, tanh e ReLU em uma mesma tarefa simples (ex.: classificaÃ§Ã£o binÃ¡ria). Qual ativaÃ§Ã£o levou a melhor convergÃªncia?

---

## ğŸ¯ Metodologia

### Tarefa Escolhida
**Problema XOR** - ClassificaÃ§Ã£o binÃ¡ria nÃ£o-linear

O problema XOR Ã© ideal para este experimento porque:
- Ã‰ simples mas nÃ£o-linearmente separÃ¡vel
- Requer que a rede aprenda representaÃ§Ãµes nÃ£o-triviais
- Ã‰ um benchmark clÃ¡ssico para redes neurais
- Possui soluÃ§Ã£o conhecida e determinÃ­stica

**Dados de entrada:**
```
Input (x1, x2) â†’ Output (y)
(0, 0) â†’ 0
(0, 1) â†’ 1
(1, 0) â†’ 1
(1, 1) â†’ 0
```

### Arquitetura da Rede
```
Camada de Entrada:  2 neurÃ´nios (x1, x2)
Camada Oculta:      4 neurÃ´nios (com ativaÃ§Ã£o testada)
Camada de SaÃ­da:    1 neurÃ´nio (sigmoid para probabilidade)
```

**ParÃ¢metros de treinamento:**
- Learning rate: 0.5
- Loss function: MSE (Mean Squared Error)
- Otimizador: Gradient Descent (batch)
- Ã‰pocas mÃ¡ximas: 1000
- NÃºmero de execuÃ§Ãµes: 5 (para robustez estatÃ­stica)

### FunÃ§Ãµes de AtivaÃ§Ã£o Testadas

#### 1. **Sigmoid**
```
Ïƒ(x) = 1 / (1 + e^(-x))
Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x))
```
- **Range:** (0, 1)
- **CaracterÃ­sticas:** Suave, diferenciÃ¡vel, mas sofre de vanishing gradient

#### 2. **Tanh (Tangente HiperbÃ³lica)**
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
tanh'(x) = 1 - tanhÂ²(x)
```
- **Range:** (-1, 1)
- **CaracterÃ­sticas:** Centralizada em zero, gradientes mais fortes que sigmoid

#### 3. **ReLU (Rectified Linear Unit)**
```
ReLU(x) = max(0, x)
ReLU'(x) = 1 se x > 0, 0 caso contrÃ¡rio
```
- **Range:** [0, +âˆ)
- **CaracterÃ­sticas:** NÃ£o satura, computacionalmente eficiente, pode sofrer de dying ReLU

---

## ğŸ”§ Como Executar

### Requisitos
```bash
pip install numpy matplotlib seaborn
```

### ExecuÃ§Ã£o
```bash
python activation_comparison.py
```

### SaÃ­das Geradas
1. **activation_comparison_results.png** - GrÃ¡ficos comparativos
2. **relatorio_experimento.txt** - EstatÃ­sticas detalhadas

---

## ğŸ“Š Resultados Esperados

### MÃ©tricas Avaliadas

1. **Velocidade de ConvergÃªncia**
   - NÃºmero de Ã©pocas atÃ© atingir 100% de acurÃ¡cia
   - MÃ©trica principal para determinar qual ativaÃ§Ã£o Ã© "melhor"

2. **Perda (Loss) ao Longo do Tempo**
   - QuÃ£o rapidamente o erro diminui
   - Indicador de estabilidade do treinamento

3. **AcurÃ¡cia ao Longo do Tempo**
   - ProgressÃ£o do aprendizado
   - Suavidade da curva de aprendizado

4. **Magnitude dos Gradientes**
   - Detecta vanishing/exploding gradients
   - Importante para entender estabilidade

### InterpretaÃ§Ã£o dos GrÃ¡ficos

#### GrÃ¡fico 1: Perda ao Longo do Treinamento
- **Eixo Y (log):** Loss (MSE)
- **Observar:** Qual curva decresce mais rapidamente
- **InterpretaÃ§Ã£o:** Descida mais rÃ¡pida = convergÃªncia mais rÃ¡pida

#### GrÃ¡fico 2: AcurÃ¡cia ao Longo do Treinamento
- **Eixo Y:** AcurÃ¡cia (%)
- **Observar:** Qual atinge 100% primeiro
- **InterpretaÃ§Ã£o:** MÃ©trica direta de desempenho

#### GrÃ¡fico 3: Magnitude dos Gradientes
- **Eixo Y (log):** Norma do gradiente
- **Observar:** Estabilidade e magnitude
- **InterpretaÃ§Ã£o:** 
  - Muito baixo = vanishing gradient
  - Muito alto = exploding gradient
  - EstÃ¡vel = bom fluxo de informaÃ§Ã£o

#### GrÃ¡fico 4: Box Plot de ConvergÃªncia
- **Observar:** Mediana, quartis, outliers
- **InterpretaÃ§Ã£o:** ConsistÃªncia entre diferentes execuÃ§Ãµes

---

## ğŸ“ Base TeÃ³rica

### Por que ReLU geralmente converge mais rÃ¡pido?

1. **NÃ£o saturaÃ§Ã£o:** 
   - ReLU nÃ£o satura para valores positivos
   - Derivada constante = 1 para x > 0
   - NÃ£o sofre de vanishing gradient

2. **Sparsidade:**
   - NeurÃ´nios com entrada negativa nÃ£o ativam
   - Rede mais eficiente e especializada

3. **Computacionalmente eficiente:**
   - OperaÃ§Ã£o simples (max)
   - Derivada trivial de calcular

### Por que Tanh Ã© melhor que Sigmoid?

1. **CentralizaÃ§Ã£o em zero:**
   - SaÃ­das entre -1 e 1
   - Facilita aprendizado nas camadas seguintes
   - Gradientes melhor balanceados

2. **Gradientes mais fortes:**
   - Derivada mÃ¡xima = 1 (em x=0)
   - Sigmoid tem derivada mÃ¡xima = 0.25
   - Fluxo de gradiente 4x melhor

### Por que Sigmoid tem dificuldades?

1. **Vanishing Gradient:**
   - Derivada muito pequena nas extremidades
   - Dificulta aprendizado em redes profundas
   - SaturaÃ§Ã£o em valores extremos

2. **NÃ£o centralizada:**
   - SaÃ­das sempre positivas [0,1]
   - Pode causar zigzagging no gradiente descent
   - ConvergÃªncia mais lenta

---

## ğŸ“ˆ AnÃ¡lise dos Resultados

### Ranking Esperado (do melhor ao pior)

**1Âº lugar: ReLU** âš¡
- ConvergÃªncia mais rÃ¡pida (~50-150 Ã©pocas)
- Gradientes estÃ¡veis
- Sem saturaÃ§Ã£o

**2Âº lugar: Tanh** ğŸŒŠ
- ConvergÃªncia moderada (~100-300 Ã©pocas)
- Melhor que sigmoid
- CentralizaÃ§Ã£o ajuda

**3Âº lugar: Sigmoid** ğŸŒ
- ConvergÃªncia mais lenta (~300-600 Ã©pocas)
- Vanishing gradient
- Menos eficiente

### Fatores que Podem Afetar os Resultados

1. **InicializaÃ§Ã£o dos pesos:**
   - He initialization para ReLU
   - Xavier initialization para Sigmoid/Tanh

2. **Learning rate:**
   - Valor fixo (0.5) pode nÃ£o ser Ã³timo para todas

3. **Problema especÃ­fico:**
   - XOR Ã© relativamente simples
   - Resultados podem variar em problemas mais complexos

---

## ğŸ”¬ Experimentos Adicionais (Opcional)

### Para Aprofundar

1. **Variar Learning Rate:**
   ```python
   for lr in [0.1, 0.5, 1.0]:
       results = run_experiment(learning_rate=lr)
   ```

2. **Arquiteturas Diferentes:**
   ```python
   # Testar: 2â†’8â†’1, 2â†’16â†’1, 2â†’4â†’4â†’1
   ```

3. **Outros Problemas:**
   - Moons dataset
   - Circles dataset
   - ClassificaÃ§Ã£o multi-classe

---

## ğŸ“š ReferÃªncias

1. **Material Stanford:** [Neural Networks Slides](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)

2. **Papers ClÃ¡ssicos:**
   - Glorot & Bengio (2010) - "Understanding the difficulty of training deep feedforward neural networks"
   - He et al. (2015) - "Delving Deep into Rectifiers: Surpassing Human-Level Performance"
   - LeCun et al. (2012) - "Efficient BackProp"

3. **Livros:**
   - Goodfellow, Bengio & Courville - "Deep Learning" (Cap. 6)
   - Nielsen - "Neural Networks and Deep Learning" (Cap. 3)
