# QuestÃ£o 4: Word Embeddings - ExplicaÃ§Ã£o e ImplementaÃ§Ã£o

## ğŸ“š IntroduÃ§Ã£o

Este documento explica e implementa **embeddings iniciais para palavras**, alÃ©m de discutir como embeddings diferem de features construÃ­das manualmente.

---

## ğŸ¯ O que sÃ£o Word Embeddings?

**Word embeddings** sÃ£o representaÃ§Ãµes vetoriais densas de palavras em um espaÃ§o contÃ­nuo de dimensÃ£o fixa. Cada palavra Ã© mapeada para um vetor de nÃºmeros reais, onde palavras com significados similares tÃªm representaÃ§Ãµes vetoriais prÃ³ximas.

### CaracterÃ­sticas principais:

- **DimensÃ£o fixa**: Geralmente entre 50-300 dimensÃµes (independente do tamanho do vocabulÃ¡rio)
- **Vetores densos**: Todos os valores sÃ£o significativos (nÃ£o esparsos)
- **Aprendidos automaticamente**: ExtraÃ­dos dos dados durante o treinamento
- **Capturam semÃ¢ntica**: Palavras similares tÃªm vetores similares

### Exemplo conceitual:

```
VocabulÃ¡rio: ["rei", "rainha", "homem", "mulher"]
DimensÃ£o dos embeddings: 3

rei    â†’ [0.8, 0.3, 0.1]
rainha â†’ [0.7, 0.4, 0.2]
homem  â†’ [0.6, 0.1, 0.1]
mulher â†’ [0.5, 0.2, 0.2]
```

---

## ğŸ”§ InicializaÃ§Ã£o de Embeddings

Os embeddings sÃ£o tipicamente inicializados aleatoriamente e depois refinados durante o treinamento. TrÃªs mÃ©todos comuns:

### 1. **InicializaÃ§Ã£o AleatÃ³ria Uniforme**
```python
embeddings = np.random.uniform(-1, 1, (vocab_size, embedding_dim))
```
- Valores entre -1 e 1
- Simples e eficaz

### 2. **InicializaÃ§Ã£o Xavier/Glorot**
```python
limit = np.sqrt(6 / (vocab_size + embedding_dim))
embeddings = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))
```
- MantÃ©m variÃ¢ncia consistente
- Boa para treinamento com gradiente

### 3. **InicializaÃ§Ã£o Normal**
```python
embeddings = np.random.normal(0, 0.1, (vocab_size, embedding_dim))
```
- DistribuiÃ§Ã£o gaussiana
- Desvio padrÃ£o pequeno (0.01 - 0.1)

---

## ğŸ†š Embeddings vs Features Manuais

### **Features Manuais (One-Hot Encoding)**

One-hot encoding Ã© a representaÃ§Ã£o tradicional de palavras:

```
VocabulÃ¡rio: ["gato", "cachorro", "carro"]

gato     â†’ [1, 0, 0]
cachorro â†’ [0, 1, 0]
carro    â†’ [0, 0, 1]
```

#### Problemas:

| Problema | DescriÃ§Ã£o |
|----------|-----------|
| **Alta dimensionalidade** | DimensÃ£o = tamanho do vocabulÃ¡rio (pode ser 10.000+) |
| **Esparsidade** | Apenas um valor Ã© 1, todos outros sÃ£o 0 |
| **Sem semÃ¢ntica** | Todas palavras sÃ£o ortogonais (similaridade = 0) |
| **NÃ£o generaliza** | NÃ£o captura relaÃ§Ãµes entre palavras |
| **Crescimento** | Cresce linearmente com novo vocabulÃ¡rio |

#### Exemplo de problema semÃ¢ntico:

```python
similaridade("gato", "cachorro") = 0.0  # Ambos sÃ£o animais, mas one-hot nÃ£o captura isso!
similaridade("gato", "carro")    = 0.0  # Mesmo valor, apesar de serem completamente diferentes
```

---

### **Word Embeddings**

RepresentaÃ§Ã£o densa e contÃ­nua:

```
VocabulÃ¡rio: ["gato", "cachorro", "carro"]
DimensÃ£o: 3

gato     â†’ [0.8, 0.7, 0.1]  (apÃ³s treinamento)
cachorro â†’ [0.7, 0.8, 0.2]  (similar a gato)
carro    â†’ [0.1, 0.2, 0.9]  (diferente)
```

#### Vantagens:

| Vantagem | DescriÃ§Ã£o |
|----------|-----------|
| **Dimensionalidade fixa** | Sempre mesmo tamanho (ex: 100), nÃ£o importa o vocabulÃ¡rio |
| **Densidade** | Todos valores sÃ£o significativos |
| **SemÃ¢ntica** | Palavras similares tÃªm vetores similares |
| **GeneralizaÃ§Ã£o** | Transfere conhecimento entre palavras similares |
| **OperaÃ§Ãµes algÃ©bricas** | Permite aritmÃ©tica vetorial (ex: rei - homem + mulher â‰ˆ rainha) |

#### Exemplo de captura semÃ¢ntica:

```python
similaridade("gato", "cachorro") = 0.87  # Alto! Ambos sÃ£o animais
similaridade("gato", "carro")    = 0.23  # Baixo! Conceitos diferentes
```

---

## ğŸ“Š Tabela Comparativa

| Aspecto | One-Hot (Manual) | Embeddings |
|---------|------------------|------------|
| **DimensÃ£o** | = tamanho vocabulÃ¡rio | Fixa (50-300) |
| **Esparsidade** | Esparso (99%+ zeros) | Denso (todos valores significativos) |
| **SemÃ¢ntica** | NÃ£o captura | Captura relaÃ§Ãµes |
| **Similaridade** | Sempre 0 ou 1 | Valor contÃ­nuo [0, 1] |
| **Aprendizado** | Fixo, prÃ©-definido | Aprendido dos dados |
| **GeneralizaÃ§Ã£o** | Fraca | Forte |
| **MemÃ³ria** | Alta (vocab_size Ã— vocab_size) | Baixa (vocab_size Ã— embedding_dim) |
| **Escalabilidade** | Ruim (cresce com vocab) | Boa (dimensÃ£o fixa) |

---

## ğŸ’¡ Por que Embeddings sÃ£o Melhores?

### 1. **EficiÃªncia Computacional**
- VocabulÃ¡rio de 10.000 palavras:
  - One-hot: 10.000 dimensÃµes
  - Embeddings: 100 dimensÃµes (100x menor!)

### 2. **Captura de RelaÃ§Ãµes SemÃ¢nticas**
```
ApÃ³s treinamento:
vec("Paris") - vec("FranÃ§a") â‰ˆ vec("Roma") - vec("ItÃ¡lia")
vec("rei") - vec("homem") â‰ˆ vec("rainha") - vec("mulher")
```

### 3. **GeneralizaÃ§Ã£o**
- Modelo aprende que "gato" e "felino" sÃ£o similares
- Mesmo que "felino" apareÃ§a pouco nos dados de treino

### 4. **TransferÃªncia de Aprendizado**
- Embeddings prÃ©-treinados (Word2Vec, GloVe, FastText)
- ReutilizÃ¡veis em mÃºltiplas tarefas

---

## ğŸ”¬ Como Embeddings sÃ£o Aprendidos?

Embeddings sÃ£o aprendidos atravÃ©s de objetivos de treinamento:

### 1. **Word2Vec (Skip-gram)**
- Dada uma palavra, prediz palavras do contexto
- Exemplo: "o __gato__ estÃ¡ feliz" â†’ prediz ["o", "estÃ¡", "feliz"]

### 2. **Word2Vec (CBOW)**
- Dado contexto, prediz palavra central
- Exemplo: ["o", "estÃ¡", "feliz"] â†’ prediz "gato"

### 3. **GloVe**
- Baseado em co-ocorrÃªncias globais de palavras

### 4. **Embeddings Contextuais (BERT, GPT)**
- Embeddings variam com contexto
- "banco" (assento) vs "banco" (financeiro)

---

## ğŸš€ Como Usar a ImplementaÃ§Ã£o

### InstalaÃ§Ã£o

```bash
pip install numpy matplotlib scikit-learn
```

### Executar

```bash
python word_embeddings.py
```

### SaÃ­das

1. **ComparaÃ§Ã£o detalhada** entre one-hot e embeddings
2. **Exemplos prÃ¡ticos** com vocabulÃ¡rio realista
3. **VisualizaÃ§Ã£o 2D** dos embeddings (`embeddings_visualization.png`)
4. **DemonstraÃ§Ã£o de similaridade** entre palavras

---

## ğŸ“ˆ VisualizaÃ§Ã£o dos Embeddings

A implementaÃ§Ã£o gera um grÃ¡fico 2D usando PCA (Principal Component Analysis) para visualizar os embeddings em duas dimensÃµes:

- Palavras relacionadas aparecem prÃ³ximas
- Clusters semÃ¢nticos emergem naturalmente
- Facilita interpretaÃ§Ã£o e debugging

---

## ğŸ“ Conceitos Importantes

### **Matriz de Embeddings**

```
Shape: (vocab_size, embedding_dim)

         dim_0  dim_1  dim_2  ...  dim_n
palavra_0  0.23   0.45  -0.12  ...   0.67
palavra_1  0.34  -0.23   0.56  ...  -0.45
palavra_2 -0.12   0.78   0.23  ...   0.34
...
```

### **Lookup de Embedding**

```python
# Para obter embedding de uma palavra:
word_idx = word2idx["gato"]  # Obter Ã­ndice
embedding = embeddings[word_idx]  # Lookup na matriz
```

### **Similaridade de Cosseno**

Medida de similaridade entre dois vetores:

```
cos(A, B) = (A Â· B) / (||A|| Ã— ||B||)

Valores:
  1.0  = idÃªnticos
  0.0  = ortogonais (nÃ£o relacionados)
 -1.0  = opostos
```

---

## ğŸ“ ConclusÃ£o

### Principais Takeaways:

1. **Embeddings superam features manuais** por capturarem semÃ¢ntica
2. **InicializaÃ§Ã£o** Ã© aleatÃ³ria, refinamento vem do treinamento
3. **Dimensionalidade fixa** torna embeddings escalÃ¡veis
4. **Aprendizado automÃ¡tico** elimina necessidade de feature engineering
5. **RepresentaÃ§Ã£o densa** Ã© mais eficiente que esparsa

### Quando usar cada um:

- **One-Hot**: VocabulÃ¡rios muito pequenos, problemas categÃ³ricos simples
- **Embeddings**: NLP moderno, grandes vocabulÃ¡rios, necessidade de semÃ¢ntica

---

## ğŸ“š ReferÃªncias

- [Stanford NLP - Neural Networks](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)
- [Stanford NLP - Vector Semantics](https://web.stanford.edu/~jurafsky/slp3/slides/vector25aug.pdf)
- Mikolov et al. (2013) - "Efficient Estimation of Word Representations in Vector Space"
- Pennington et al. (2014) - "GloVe: Global Vectors for Word Representation"

---

## ğŸ› ï¸ Estrutura do CÃ³digo

```
word_embeddings.py
â”œâ”€â”€ WordEmbeddings          # Classe principal de embeddings
â”‚   â”œâ”€â”€ initialize_embeddings()  # Diferentes mÃ©todos de inicializaÃ§Ã£o
â”‚   â”œâ”€â”€ get_embedding()          # Recuperar embedding de palavra
â”‚   â”œâ”€â”€ similarity()             # Calcular similaridade
â”‚   â””â”€â”€ visualize_embeddings_2d()  # Plotar em 2D
â”‚
â”œâ”€â”€ ManualFeatures          # Classe para one-hot encoding
â”‚   â””â”€â”€ one_hot_encode()    # Criar representaÃ§Ã£o one-hot
â”‚
â”œâ”€â”€ compare_embeddings_vs_manual()  # ComparaÃ§Ã£o detalhada
â””â”€â”€ exemplo_pratico()       # DemonstraÃ§Ã£o completa
```

