# ImplementaÃ§Ã£o de Mean e Max Pooling para Embeddings

## ğŸ“ DescriÃ§Ã£o da QuestÃ£o

Dada uma matriz de embeddings de uma frase, implementar mean pooling e max pooling, e analisar qual das estratÃ©gias preserva mais informaÃ§Ã£o no exemplo.

## ğŸ¯ Objetivo

Este projeto implementa e compara duas estratÃ©gias fundamentais de pooling usadas em Processamento de Linguagem Natural (PLN) para agregar embeddings de sequÃªncias:

- **Mean Pooling**: Calcula a mÃ©dia de cada dimensÃ£o ao longo da sequÃªncia
- **Max Pooling**: Seleciona o valor mÃ¡ximo de cada dimensÃ£o ao longo da sequÃªncia

## ğŸš€ Como Executar

### Requisitos
```bash
pip install numpy matplotlib seaborn
```

### ExecuÃ§Ã£o
```bash
python pooling_implementation.py
```

## ğŸ“Š O que o cÃ³digo faz

1. **Cria uma matriz de embeddings simulada** (5 tokens Ã— 8 dimensÃµes)
2. **Aplica mean pooling e max pooling**
3. **Calcula mÃ©tricas de preservaÃ§Ã£o de informaÃ§Ã£o**
4. **Gera visualizaÃ§Ãµes comparativas** (salvas como `pooling_comparison.png`)
5. **Apresenta anÃ¡lise detalhada** de qual mÃ©todo preserva mais informaÃ§Ã£o

## ğŸ” EstratÃ©gias de Pooling

### Mean Pooling

**FÃ³rmula**: Para cada dimensÃ£o `d`:
```
result[d] = (1/n) Ã— Î£ embeddings[i][d]
```

**CaracterÃ­sticas**:
- âœ… Preserva a distribuiÃ§Ã£o geral dos valores
- âœ… Robusta a outliers
- âœ… Todos os tokens contribuem igualmente
- âŒ Pode diluir informaÃ§Ãµes importantes
- âŒ Perde valores extremos

**Quando usar**: 
- Quando todos os tokens sÃ£o igualmente importantes
- Para representaÃ§Ãµes balanceadas de documentos
- Em classificaÃ§Ã£o de sentimentos onde o contexto geral importa

### Max Pooling

**FÃ³rmula**: Para cada dimensÃ£o `d`:
```
result[d] = max(embeddings[:][d])
```

**CaracterÃ­sticas**:
- âœ… Preserva caracterÃ­sticas salientes
- âœ… Captura as features mais ativadas
- âœ… Ãštil para detecÃ§Ã£o de padrÃµes especÃ­ficos
- âŒ Ignora valores mÃ©dios e baixos
- âŒ SensÃ­vel a outliers

**Quando usar**:
- Quando features especÃ­ficas sÃ£o crÃ­ticas
- Em detecÃ§Ã£o de entidades ou keywords
- Para capturar sinais fortes em qualquer posiÃ§Ã£o

## ğŸ“ˆ AnÃ¡lise de PreservaÃ§Ã£o de InformaÃ§Ã£o

### MÃ©tricas Utilizadas

1. **VariÃ¢ncia**: Mede a dispersÃ£o dos valores
2. **Norma**: Magnitude do vetor resultante
3. **Range**: Amplitude dos valores (max - min)
4. **DistribuiÃ§Ã£o**: ComparaÃ§Ã£o visual das distribuiÃ§Ãµes

### Resultados do Exemplo

No exemplo implementado (com embeddings simulados):

- **Max Pooling** tende a preservar maior variÃ¢ncia e valores extremos
- **Mean Pooling** tende a preservar melhor a distribuiÃ§Ã£o geral

## ğŸ“ Conceitos TeÃ³ricos

### Por que usar Pooling?

Embeddings de sequÃªncias tÃªm dimensÃ£o `(sequence_length, embedding_dim)`, mas muitas tarefas precisam de vetores de tamanho fixo `(embedding_dim)`. Pooling resolve isso agregando a informaÃ§Ã£o da sequÃªncia.

### RelaÃ§Ã£o com o Material (Jurafsky & Martin)

Conforme os slides de Stanford, pooling Ã© uma operaÃ§Ã£o crucial em:
- Redes neurais para classificaÃ§Ã£o de texto
- Sentence embeddings
- ReduÃ§Ã£o de dimensionalidade temporal

## ğŸ† ConclusÃ£o

**Qual preserva mais informaÃ§Ã£o?**

A resposta depende do tipo de informaÃ§Ã£o que Ã© valiosa para a tarefa:

### Mean Pooling preserva:
- InformaÃ§Ã£o sobre a **distribuiÃ§Ã£o geral**
- TendÃªncias centrais
- ContribuiÃ§Ãµes balanceadas de todos os tokens

### Max Pooling preserva:
- InformaÃ§Ã£o sobre **caracterÃ­sticas salientes**
- Valores extremos (picos de ativaÃ§Ã£o)
- Features mais discriminativas

**Na prÃ¡tica**: Mean pooling Ã© mais comum em modelos modernos (como sentence-BERT) porque fornece representaÃ§Ãµes mais estÃ¡veis e balanceadas. Max pooling Ã© preferido quando caracterÃ­sticas especÃ­ficas sÃ£o mais importantes que o contexto geral.

## ğŸ“š ReferÃªncias

- Jurafsky & Martin - Speech and Language Processing
- [Stanford NLP Slides](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)
- Devlin et al. (2019) - BERT: Pre-training of Deep Bidirectional Transformers
- Reimers & Gurevych (2019) - Sentence-BERT

## ğŸ‘¨â€ğŸ’» Estrutura do CÃ³digo

```
pooling_implementation.py
â”œâ”€â”€ mean_pooling()          # Implementa mean pooling
â”œâ”€â”€ max_pooling()           # Implementa max pooling
â”œâ”€â”€ calcular_preservacao_informacao()  # MÃ©tricas
â”œâ”€â”€ visualizar_comparacao() # Gera grÃ¡ficos
â””â”€â”€ main()                  # ExecuÃ§Ã£o principal
```

## ğŸ–¼ï¸ VisualizaÃ§Ãµes Geradas

O script gera um arquivo `pooling_comparison.png` com 4 subplots:
1. Heatmap da matriz de embeddings original
2. ComparaÃ§Ã£o direta dos vetores resultantes
3. DistribuiÃ§Ã£o dos valores
4. AnÃ¡lise por dimensÃ£o

## ğŸ’¡ Experimente

VocÃª pode modificar a matriz de embeddings em `main()` para testar com:
- Diferentes tamanhos de sequÃªncia
- Diferentes dimensÃµes de embedding
- PadrÃµes especÃ­ficos (valores esparsos, densos, etc.)

