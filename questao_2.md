# Problema do XOR: Perceptron Simples vs. Rede de Duas Camadas

## ğŸ“‹ QuestÃ£o

**Mostre que um Ãºnico perceptron nÃ£o consegue aprender a funÃ§Ã£o XOR. Depois, implemente uma rede de duas camadas que consiga resolvÃª-la.**

---

## ğŸ¯ Objetivo

Demonstrar atravÃ©s de implementaÃ§Ã£o prÃ¡tica:
1. **Por que um perceptron simples falha** ao aprender a funÃ§Ã£o XOR
2. **Como uma rede de duas camadas (MLP)** consegue resolver o problema

---

## ğŸ“Š A FunÃ§Ã£o XOR

A funÃ§Ã£o XOR (ou exclusivo) retorna 1 apenas quando as entradas sÃ£o diferentes:

| Xâ‚ | Xâ‚‚ | Y (XOR) |
|----|----|---------| 
| 0  | 0  | 0       |
| 0  | 1  | 1       |
| 1  | 0  | 1       |
| 1  | 1  | 0       |

---

## âŒ Parte 1: Por que o Perceptron Simples Falha?

### Problema Fundamental: Separabilidade Linear

Um perceptron simples implementa a funÃ§Ã£o:

```
y = step(wâ‚xâ‚ + wâ‚‚xâ‚‚ + b)
```

Esta funÃ§Ã£o representa uma **linha reta** (ou hiperplano em dimensÃµes maiores) que divide o espaÃ§o em duas regiÃµes.

### LimitaÃ§Ã£o do Perceptron

Para classificar corretamente o XOR, seria necessÃ¡rio separar:
- **Classe 0**: pontos (0,0) e (1,1)
- **Classe 1**: pontos (0,1) e (1,0)

**Problema**: Esses pontos estÃ£o em configuraÃ§Ã£o diagonal! **NÃ£o existe uma linha reta** que possa separar essas duas classes.

### Teorema de ConvergÃªncia do Perceptron

O teorema de convergÃªncia do perceptron (Rosenblatt, 1962) garante convergÃªncia apenas para problemas **linearmente separÃ¡veis**. Como XOR nÃ£o Ã© linearmente separÃ¡vel, o perceptron nunca convergirÃ¡ para uma soluÃ§Ã£o correta.

### Resultado Esperado

O perceptron simples terÃ¡ **acurÃ¡cia â‰¤ 75%**, geralmente acertando apenas 2 ou 3 dos 4 casos.

---

## âœ… Parte 2: SoluÃ§Ã£o com Rede de Duas Camadas (MLP)

### Arquitetura da Rede

```
Input Layer (2 neurÃ´nios)
        â†“
Hidden Layer (2 neurÃ´nios, sigmoid)
        â†“
Output Layer (1 neurÃ´nio, sigmoid)
```

### Como Funciona?

1. **Camada Oculta**: Transforma o espaÃ§o de entrada de forma nÃ£o-linear
2. **RepresentaÃ§Ã£o IntermediÃ¡ria**: Cria um novo espaÃ§o onde o problema se torna linearmente separÃ¡vel
3. **Camada de SaÃ­da**: Realiza a classificaÃ§Ã£o final no espaÃ§o transformado

### Algoritmo de Treinamento

**Forward Pass:**
```
h = sigmoid(Wâ‚X + bâ‚)  # Camada oculta
y = sigmoid(Wâ‚‚h + bâ‚‚)   # Camada de saÃ­da
```

**Backward Pass (Backpropagation):**
```
Î´â‚‚ = (y - target) Ã— sigmoid'(y)       # Erro da saÃ­da
Î´â‚ = (Î´â‚‚ Ã— Wâ‚‚) Ã— sigmoid'(h)          # Erro da camada oculta

# Atualizar pesos
Wâ‚‚ = Wâ‚‚ - lr Ã— Î´â‚‚ Ã— h
Wâ‚ = Wâ‚ - lr Ã— Î´â‚ Ã— X
```

### Resultado Esperado

O MLP alcanÃ§a **acurÃ¡cia de 100%**, classificando corretamente todos os 4 casos de XOR.

---

## ğŸš€ Como Executar

### Requisitos

```bash
pip install numpy matplotlib
```

### ExecuÃ§Ã£o

```bash
python xor_neural_networks.py
```

### SaÃ­das Geradas

O programa gera:

1. **Resultados no Terminal**:
   - Tabela verdade do XOR
   - Desempenho do perceptron simples (falha)
   - Desempenho do MLP (sucesso)

2. **GrÃ¡ficos PNG**:
   - `perceptron_simples.png` - Fronteira de decisÃ£o do perceptron (inadequada)
   - `mlp_xor.png` - Fronteira de decisÃ£o do MLP (correta)
   - `convergencia_arquitetura.png` - Curva de convergÃªncia e arquitetura

---

## ğŸ“ˆ Resultados Esperados

### Perceptron Simples
- **AcurÃ¡cia**: ~50-75%
- **ConclusÃ£o**: NÃ£o converge para soluÃ§Ã£o correta
- **RazÃ£o**: XOR nÃ£o Ã© linearmente separÃ¡vel

### MLP (Duas Camadas)
- **AcurÃ¡cia**: 100%
- **Ã‰pocas necessÃ¡rias**: ~5000
- **ConclusÃ£o**: Resolve XOR perfeitamente
- **RazÃ£o**: Camada oculta cria representaÃ§Ãµes nÃ£o-lineares

---

## ğŸ§  Conceitos Importantes

### 1. Separabilidade Linear
Um problema Ã© **linearmente separÃ¡vel** se existe um hiperplano que separa perfeitamente as classes.

**Exemplos**:
- âœ… AND, OR, NOT â†’ Linearmente separÃ¡veis
- âŒ XOR, XNOR â†’ NÃ£o linearmente separÃ¡veis

### 2. RepresentaÃ§Ãµes NÃ£o-Lineares
A camada oculta do MLP aprende a transformar o espaÃ§o de entrada, criando caracterÃ­sticas que tornam o problema separÃ¡vel na nova representaÃ§Ã£o.

### 3. Teorema da AproximaÃ§Ã£o Universal
Uma rede neural com pelo menos uma camada oculta pode aproximar qualquer funÃ§Ã£o contÃ­nua, dado neurÃ´nios suficientes (Cybenko, 1989).

### 4. Backpropagation
Algoritmo para calcular gradientes eficientemente em redes multicamadas, permitindo o treinamento atravÃ©s de otimizaÃ§Ã£o por gradiente descendente.

---

## ğŸ“š ReferÃªncias

1. **Rosenblatt, F. (1958)**. "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain"

2. **Minsky, M. & Papert, S. (1969)**. "Perceptrons" - Demonstrou matematicamente as limitaÃ§Ãµes do perceptron

3. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986)**. "Learning representations by back-propagating errors"

4. **Cybenko, G. (1989)**. "Approximation by superpositions of a sigmoidal function"

5. **Material do Curso**: [Neural Networks - Stanford](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)

---

## ğŸ’¡ ConclusÃµes

### Por que Perceptron Falha?
- Implementa apenas separaÃ§Ã£o linear
- XOR requer separaÃ§Ã£o nÃ£o-linear
- LimitaÃ§Ã£o fundamental da arquitetura

### Por que MLP Funciona?
- Camada oculta cria representaÃ§Ãµes nÃ£o-lineares
- Transforma o espaÃ§o tornando-o linearmente separÃ¡vel
- Backpropagation permite aprender os pesos adequados

### ImplicaÃ§Ãµes HistÃ³ricas
O problema do XOR foi crucial na histÃ³ria das redes neurais:
- **1969**: Minsky & Papert mostraram limitaÃ§Ãµes do perceptron â†’ "AI Winter"
- **1986**: Rumelhart et al. introduziram backpropagation â†’ Renascimento das redes neurais
- **Hoje**: Redes profundas (deep learning) resolvem problemas muito mais complexos

---

## ğŸ‘¨â€ğŸ’» Estrutura do CÃ³digo

```
xor_neural_networks.py
â”œâ”€â”€ Dados do XOR
â”œâ”€â”€ FunÃ§Ãµes de AtivaÃ§Ã£o (sigmoid, step)
â”œâ”€â”€ Classe SimplePerceptron
â”‚   â”œâ”€â”€ predict()
â”‚   â”œâ”€â”€ train()
â”‚   â””â”€â”€ evaluate()
â”œâ”€â”€ Classe TwoLayerNetwork (MLP)
â”‚   â”œâ”€â”€ forward()
â”‚   â”œâ”€â”€ backward()
â”‚   â”œâ”€â”€ train()
â”‚   â””â”€â”€ predict()
â”œâ”€â”€ Treinamento e AvaliaÃ§Ã£o
â””â”€â”€ VisualizaÃ§Ãµes
```

---

## ğŸ“ Aprendizados Principais

1. **Nem todos os problemas sÃ£o linearmente separÃ¡veis**
2. **Camadas ocultas adicionam poder computacional**
3. **NÃ£o-linearidade Ã© essencial para problemas complexos**
4. **Backpropagation permite treinar redes profundas**
5. **AtÃ© problemas simples podem requerer arquiteturas nÃ£o-triviais**

---

## ğŸ“ Notas Adicionais

- O nÃºmero mÃ­nimo de neurÃ´nios na camada oculta para resolver XOR Ã© **2**
- Taxa de aprendizado tÃ­pica: **0.5 para MLP, 0.1 para perceptron**
- FunÃ§Ã£o sigmoid Ã© usada para introduzir nÃ£o-linearidade
- Para problemas mais complexos, podem ser necessÃ¡rias mais camadas e neurÃ´nios

