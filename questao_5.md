# MLP de 2 Camadas: Forward Pass e Backpropagation Manual

Este projeto demonstra o cÃ¡lculo manual de **forward pass** (propagaÃ§Ã£o para frente) e **backpropagation** (propagaÃ§Ã£o para trÃ¡s) em uma rede neural Multi-Layer Perceptron (MLP) de 2 camadas.

## ğŸ“š Baseado no Material

Este cÃ³digo Ã© baseado nos slides de Stanford sobre Redes Neurais:
- **Material**: [Neural Networks - Stanford](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)
- **Conceitos cobertos**: Forward pass, Backward differentiation, Computation graphs, Chain rule

## ğŸ—ï¸ Arquitetura da Rede

```
Entrada (3 neurÃ´nios)
    â†“ W[1], b[1]
Camada Oculta (2 neurÃ´nios) - AtivaÃ§Ã£o: ReLU
    â†“ W[2], b[2]
SaÃ­da (1 neurÃ´nio) - AtivaÃ§Ã£o: Sigmoid
    â†“
Loss: Binary Cross-Entropy
```

### DimensÃµes dos Pesos

- **W[1]**: [2 Ã— 3] - Conecta entrada (3) Ã  camada oculta (2)
- **b[1]**: [2 Ã— 1] - Bias da camada oculta
- **W[2]**: [1 Ã— 2] - Conecta camada oculta (2) Ã  saÃ­da (1)
- **b[2]**: [1 Ã— 1] - Bias da camada de saÃ­da

## ğŸ”¢ EquaÃ§Ãµes MatemÃ¡ticas

### Forward Pass

1. **Camada Oculta**:
   ```
   z[1] = W[1] @ x + b[1]
   a[1] = ReLU(z[1]) = max(0, z[1])
   ```

2. **Camada de SaÃ­da**:
   ```
   z[2] = W[2] @ a[1] + b[2]
   a[2] = sigmoid(z[2]) = 1 / (1 + e^(-z[2]))
   Å· = a[2]
   ```

3. **Loss Function** (Binary Cross-Entropy):
   ```
   L = -[y*log(Å·) + (1-y)*log(1-Å·)]
   ```

### Backward Pass (Backpropagation)

Usa a **Chain Rule** para calcular os gradientes:

1. **Derivada da Loss**:
   ```
   âˆ‚L/âˆ‚a[2] = a[2] - y
   ```

2. **Camada de SaÃ­da**:
   ```
   âˆ‚a[2]/âˆ‚z[2] = sigmoid'(z[2]) = a[2] * (1 - a[2])
   âˆ‚L/âˆ‚z[2] = âˆ‚L/âˆ‚a[2] * âˆ‚a[2]/âˆ‚z[2]
   
   âˆ‚L/âˆ‚W[2] = âˆ‚L/âˆ‚z[2] @ a[1]áµ€
   âˆ‚L/âˆ‚b[2] = âˆ‚L/âˆ‚z[2]
   ```

3. **Camada Oculta**:
   ```
   âˆ‚L/âˆ‚a[1] = W[2]áµ€ @ âˆ‚L/âˆ‚z[2]
   âˆ‚a[1]/âˆ‚z[1] = ReLU'(z[1]) = {1 se z[1] > 0, 0 caso contrÃ¡rio}
   âˆ‚L/âˆ‚z[1] = âˆ‚L/âˆ‚a[1] * âˆ‚a[1]/âˆ‚z[1]
   
   âˆ‚L/âˆ‚W[1] = âˆ‚L/âˆ‚z[1] @ xáµ€
   âˆ‚L/âˆ‚b[1] = âˆ‚L/âˆ‚z[1]
   ```

4. **AtualizaÃ§Ã£o dos Pesos** (Gradient Descent):
   ```
   W_novo = W_antigo - Î· * âˆ‚L/âˆ‚W
   b_novo = b_antigo - Î· * âˆ‚L/âˆ‚b
   ```
   onde Î· Ã© a taxa de aprendizado (learning rate).

## ğŸš€ Como Executar

### Requisitos

```bash
pip install numpy
```

### ExecuÃ§Ã£o

```bash
python mlp_manual.py
```

## ğŸ“Š Exemplo de SaÃ­da

O programa executarÃ¡ uma iteraÃ§Ã£o completa de treinamento e exibirÃ¡:

1. **Forward Pass**: Valores intermediÃ¡rios de cada camada
2. **Loss**: Valor da funÃ§Ã£o de perda
3. **Backward Pass**: Todos os gradientes calculados
4. **AtualizaÃ§Ã£o**: Pesos antes e depois da atualizaÃ§Ã£o

```
=====================================
FORWARD PASS - PROPAGAÃ‡ÃƒO PARA FRENTE
=====================================

Input x: [0.5 0.6 0.1]

--- CAMADA 1 (Input -> Hidden) ---
z[1] = W[1] @ x + b[1]
z[1] = [[0.2 0.3 0.1]
        [0.4 -0.2 0.5]] @ [0.5 0.6 0.1] + [0.1 0.2]
z[1] = [0.39 0.38]

a[1] = ReLU(z[1]) = [0.39 0.38]

--- CAMADA 2 (Hidden -> Output) ---
...
```

## ğŸ”‘ Conceitos-Chave

### 1. Forward Pass
Processo de calcular a saÃ­da da rede propagando os dados de entrada atravÃ©s das camadas, da esquerda para direita.

### 2. Backpropagation
Algoritmo para calcular os gradientes da funÃ§Ã£o de perda em relaÃ§Ã£o aos pesos, propagando o erro de volta atravÃ©s da rede, da direita para esquerda.

### 3. Chain Rule
Regra do cÃ¡lculo que permite calcular derivadas de funÃ§Ãµes compostas:
```
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a * âˆ‚a/âˆ‚z * âˆ‚z/âˆ‚W
```

### 4. FunÃ§Ãµes de AtivaÃ§Ã£o

- **ReLU**: `max(0, z)`
  - Simples e eficiente
  - Evita o problema de vanishing gradient
  - Derivada: 1 se z > 0, 0 caso contrÃ¡rio

- **Sigmoid**: `1 / (1 + e^(-z))`
  - SaÃ­da entre 0 e 1
  - Ideal para classificaÃ§Ã£o binÃ¡ria
  - Derivada: Ïƒ(z) * (1 - Ïƒ(z))

### 5. Loss Function

**Binary Cross-Entropy**: Mede a diferenÃ§a entre a distribuiÃ§Ã£o de probabilidade prevista e a real para classificaÃ§Ã£o binÃ¡ria.

## ğŸ“ AplicaÃ§Ã£o PrÃ¡tica

Este cÃ³digo Ã© Ãºtil para:

1. âœ… **Entender backpropagation**: Ver cada passo do cÃ¡lculo
2. âœ… **Verificar implementaÃ§Ãµes**: Comparar com frameworks
3. âœ… **Estudar para provas**: Material didÃ¡tico com cÃ¡lculos completos
4. âœ… **Debug**: Identificar problemas em redes neurais

## ğŸ“– ReferÃªncias

1. Stanford CS224N - Neural Networks and Neural Language Models
2. Rumelhart, Hinton, Williams (1986) - Learning representations by back-propagating errors
3. Goodfellow, Bengio, Courville - Deep Learning Book

## ğŸ¤ ModificaÃ§Ãµes Sugeridas

Para explorar mais:

1. **Adicionar mais camadas**: Criar um MLP de 3+ camadas
2. **Testar outras ativaÃ§Ãµes**: tanh, Leaky ReLU, etc.
3. **MÃºltiplas iteraÃ§Ãµes**: Loop de treinamento completo
4. **Diferentes loss functions**: MSE, MAE, etc.
5. **Mini-batches**: Processar mÃºltiplos exemplos de uma vez
6. **RegularizaÃ§Ã£o**: L1, L2, Dropout


